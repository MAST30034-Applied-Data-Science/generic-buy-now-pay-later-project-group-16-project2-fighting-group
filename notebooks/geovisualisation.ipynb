{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from shapely.wkt import loads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "from dbfread import DBF\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.error import HTTPError\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/15 15:21:32 WARN Utils: Your hostname, hexiangyideMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.13.189.29 instead (on interface en0)\n",
      "22/10/15 15:21:32 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/15 15:21:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/15 15:21:34 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "22/10/15 15:21:34 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "# Download SA2 2021 information\n",
    "url = \"https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA2_2021_AUST_SHP_GDA2020.zip\" \n",
    "# year-month.parquet\n",
    "output_dir = \"../data/outer/2021sa2_shapefile.zip\"\n",
    "# download\n",
    "urlretrieve(url, output_dir) \n",
    "print(\"complete\")\n",
    "\n",
    "files = zipfile.ZipFile('../data/outer/2021sa2_shapefile.zip','r')\n",
    "for file in files.namelist():\n",
    "    files.extract(file, f\"../data/outer/2021sa2_shapefile\")\n",
    "\n",
    "# Download shapefile for each SA2 region\n",
    "def get_shapefile(url):\n",
    "    geojson_option = \"?_profile=oai&_mediatype=application/geo+json\"\n",
    "    try:\n",
    "        shape = str(gpd.read_file(url + geojson_option).iat[0,-1])\n",
    "    except HTTPError:\n",
    "        shape = \"\"\n",
    "    return shape\n",
    "\n",
    "get_shapefile_udf = udf(lambda a: get_shapefile(a),StringType())\n",
    "\n",
    "path = r'../data/outer/2021sa2_shapefile/SA2_2021_AUST_GDA2020.dbf' \n",
    "table = DBF(path)\n",
    "sa2_pd_temp = pd.DataFrame(iter(table))\n",
    "sa2_2021_temp = spark.createDataFrame(sa2_pd_temp) \n",
    "\n",
    "sa2_2021_vic = sa2_2021_temp.filter(F.col(\"STE_NAME21\")==\"Victoria\")\n",
    "sa2_2021_vic_w_geo = sa2_2021_vic.withColumn(\"geometry\", get_shapefile_udf(F.col(\"LOCI_URI21\")))\n",
    "sa2_2021_vic_w_geo = sa2_2021_vic_w_geo.select(\"SA2_CODE21\", \"SA2_NAME21\", \"geometry\")\n",
    "#sa2_2021_vic_w_geo.write.option(\"maxRecordsPerFile\", 1).mode(\"overwrite\").parquet(\"data/external/SA2_2021_VIC_shapefile.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#geospark = spark.read.parquet('../data/external/SA2_2021_VIC_shapefile.parquet/')\n",
    "geospark = sa2_2021_vic_w_geo\n",
    "census = spark.read.parquet('../data/curated/final_census.parquet')\n",
    "vic_data = geospark.join(census, geospark.SA2_CODE21 == census.SA2_CODE_2021, \"left\").drop(census.SA2_CODE_2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SA2 to do the visualizaton\n",
    "def visualisation(col_name, vic_data,geospark, title):\n",
    "    data = vic_data.groupBy(\"SA2_CODE21\").agg(F.avg(col_name).alias(col_name))\n",
    "    pdf = geospark.select(\"SA2_CODE21\", \"geometry\").toPandas()\n",
    "    pdf['geometry'] = gpd.GeoSeries.from_wkt(pdf['geometry'])\n",
    "    gdf = gpd.GeoDataFrame(pdf, geometry='geometry')\n",
    "    geoJSON = gdf[['SA2_CODE21', 'geometry']].drop_duplicates('SA2_CODE21').to_json()\n",
    "    m = folium.Map(location=[-37.84, 144.95], tiles=\"Stamen Terrain\", zoom_start=10)\n",
    "    # refer to the folium documentations on how to plot aggregated data.\n",
    "    c = folium.Choropleth(\n",
    "        geo_data=geoJSON, # geoJSON \n",
    "        name='choropleth', # name of plot\n",
    "        data=data.toPandas(), # data source\n",
    "        columns=['SA2_CODE21',col_name], # the columns required\n",
    "        key_on='properties.SA2_CODE21', # this is from the geoJSON's properties\n",
    "        fill_color='YlOrRd', # color scheme\n",
    "        nan_fill_color='black',\n",
    "        legend_name=title\n",
    "    )\n",
    "\n",
    "    map = c.add_to(m)\n",
    "\n",
    "    map.save('../plots/foliumChoropleth_'+col_name+'.html')\n",
    "    map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the postcode, draw the proportion of the age group between 20-64 in total number of people in that region and the proportion of people earning above the Australian weekly median income ($1200) to the total number of people. The darker the color on the geo graph, the higher the percentage and the higher the consumption level of the people in the region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "visualisation(\"income_percentage\", vic_data, geospark,\"percentage of people above medium income by SA2\")\n",
    "visualisation(\"age_percentage\", vic_data, geospark,\"age percentage between 25-60\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ads')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e45bdfcba1031802b2f7a315f02d536e51553942a60e4ee4be74aec82814bfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
