{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from dbfread import DBF\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.error import HTTPError\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/18 15:54:10 WARN Utils: Your hostname, hexiangyideMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.13.177.117 instead (on interface en0)\n",
      "22/10/18 15:54:10 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/18 15:54:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download census data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_relative_dir = '../data/'\n",
    "\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)\n",
    "    \n",
    "for target_dir in ('external','outer'):\n",
    "    if not os.path.exists(output_relative_dir + target_dir):\n",
    "        os.makedirs(output_relative_dir + target_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download census data\n",
    "url = \"https://www.abs.gov.au/census/find-census-data/datapacks/download/2021_GCP_SA2_for_AUS_short-header.zip\"#year-month.parquet\n",
    "output_dir = \"../data/outer/census.zip\"\n",
    "urlretrieve(url, output_dir) \n",
    "\n",
    "files = zipfile.ZipFile('../data/outer/census.zip','r')\n",
    "\n",
    "for file in files.namelist():\n",
    "    files.extract(file, f\"../data/outer/census\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/18 15:54:28 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "types = [\"A\", \"B\"]\n",
    "for type in types:\n",
    "    G04= spark.read.option(\"header\",True).csv(f'../data/outer/census/2021 Census GCP Statistical Area 2 for AUS/2021Census_G04{type}_AUST_SA2.csv') #read the parquet \n",
    "    G04 = G04.na.drop(subset=G04.columns)\n",
    "    G04.write.mode(\"overwrite\").parquet(f\"../data/external/census_data_2021_04{type}.parquet\")\n",
    "\n",
    "types = [\"A\", \"B\", \"C\"]\n",
    "for type in types:\n",
    "    G17= spark.read.option(\"header\",True).csv(f'../data/outer/census/2021 Census GCP Statistical Area 2 for AUS/2021Census_G17{type}_AUST_SA2.csv') #read the parquet \n",
    "    G17 = G17.na.drop(subset=G17.columns)\n",
    "\n",
    "    G17.write.mode(\"overwrite\").parquet(f\"../data/external/census_data_2021_17{type}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download postcode and SA2 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://github.com/matthewproctor/australianpostcodes/zipball/master\"\n",
    "output_dir = \"../data/outer/total_pto2.zip\"\n",
    "urlretrieve(url, output_dir) \n",
    "\n",
    "files = zipfile.ZipFile('../data/outer/total_pto2.zip','r')\n",
    "for file in files.namelist():\n",
    "    files.extract(file, f\"../data/outer/total_pto2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_sa2= spark.read.option(\"header\",True).csv('../data/outer/total_pto2/matthewproctor-australianpostcodes-6f8a994/australian_postcodes.csv') \n",
    "post_sa2 = post_sa2.select(\"id\", \"postcode\", \"Lat_precise\", \"Long_precise\", \"SA2_MAINCODE_2016\")\n",
    "post_sa2 = post_sa2.na.drop()\n",
    "\n",
    "# get the average latitude and longitude of the postcode if it have duplicates\n",
    "post_sa2 = post_sa2\\\n",
    "    .withColumn(\"Lat_double\", post_sa2[\"Lat_precise\"].cast(\"double\"))\\\n",
    "    .withColumn(\"Long_double\", post_sa2[\"Long_precise\"].cast(\"double\"))\n",
    "post_sa2 = post_sa2\\\n",
    "    .groupBy(\"postcode\", \"SA2_MAINCODE_2016\")\\\n",
    "    .agg(F.avg(\"Lat_double\").alias(\"avg_lat\"), F.avg(\"Long_double\").alias(\"avg_long\"))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download SA2 2021 information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "complete\n"
     ]
    }
   ],
   "source": [
    "url = \"https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA2_2021_AUST_SHP_GDA2020.zip\" \n",
    "output_dir = \"../data/outer/2021sa2_shapefile.zip\"\n",
    "\n",
    "urlretrieve(url, output_dir) \n",
    "print(\"complete\")\n",
    "\n",
    "files = zipfile.ZipFile('../data/outer/2021sa2_shapefile.zip','r')\n",
    "for file in files.namelist():\n",
    "    files.extract(file, f\"../data/outer/2021sa2_shapefile\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download shapefile for each SA2 region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shapefile(url):\n",
    "    geojson_option = \"?_profile=oai&_mediatype=application/geo+json\"\n",
    "    try:\n",
    "        shape = str(gpd.read_file(url + geojson_option).iat[0,-1])\n",
    "    except HTTPError:\n",
    "        shape = \"\"\n",
    "    return shape\n",
    "\n",
    "get_shapefile_udf = udf(lambda a: get_shapefile(a),StringType())\n",
    "\n",
    "path = r'../data/outer/2021sa2_shapefile/SA2_2021_AUST_GDA2020.dbf' \n",
    "table = DBF(path)\n",
    "sa2_pd_temp = pd.DataFrame(iter(table))\n",
    "sa2_2021_temp = spark.createDataFrame(sa2_pd_temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Extract only the data from Victoria state for visualisation \n",
    "sa2_2021_vic = sa2_2021_temp.filter(F.col(\"STE_NAME21\")==\"Victoria\")\n",
    "sa2_2021_vic_w_geo = sa2_2021_vic.withColumn(\"geometry\", get_shapefile_udf(F.col(\"LOCI_URI21\")))\n",
    "sa2_2021_vic_w_geo = sa2_2021_vic_w_geo.select(\"SA2_CODE21\", \"SA2_NAME21\", \"geometry\")\n",
    "sa2_2021_vic_w_geo.write.mode('overwrite').parquet(\"../data/outer/sa2_2021_vic_w_geo.parquet\")\n",
    "\n",
    "# Download file with 2016 SA2 info and 2021 SA2 info\n",
    "url = \"https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/correspondences/CG_SA2_2016_SA2_2021.csv\"\n",
    "output_dir = \"../data/outer/correspondences.csv\"\n",
    "urlretrieve(url, output_dir) \n",
    "\n",
    "correspondences = spark.read.option(\"header\",True).csv('../data/outer/correspondences.csv') #read the parquet \n",
    "correspondences = correspondences.na.drop()\n",
    "\n",
    "# Find 2016 post code and 2021 sa2 correspondence\n",
    "post_sa2_2021 = correspondences.join(post_sa2,correspondences.SA2_MAINCODE_2016 == post_sa2.SA2_MAINCODE_2016,\"left\") \n",
    "\n",
    "# JOIN tables: this will be the final SA2 and postcode correnpondence file\n",
    "post_sa2_2021 = post_sa2_2021.drop(\"SA2_NAME_2016\", \"SA2_MAINCODE_2016\")\n",
    "post_sa2_2021.write.mode('overwrite').parquet(\"../data/external/postcode_sa2_conrrespondences.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process census age and income data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the external census datasets\n",
    "census_04A_age = spark.read.parquet('../data/external/census_data_2021_04A.parquet')\n",
    "census_04B_age = spark.read.parquet('../data/external/census_data_2021_04B.parquet')\n",
    "census_17A_income = spark.read.parquet('../data/external/census_data_2021_17A.parquet')\n",
    "census_17B_income = spark.read.parquet('../data/external/census_data_2021_17B.parquet')\n",
    "census_17C_income = spark.read.parquet('../data/external/census_data_2021_17C.parquet')\n",
    "postcode = post_sa2\n",
    "\n",
    "# Merge the age dataframe\n",
    "Age_data = census_04A_age.join(census_04B_age, census_04A_age.SA2_CODE_2021 == census_04B_age.SA2_CODE_2021).drop(census_04B_age.SA2_CODE_2021)\n",
    "\n",
    "# Select the column in the age dataframe (18-65) and calculate the sum\n",
    "Age_data_select = Age_data.select(Age_data.columns[0::18])\n",
    "Age_data_total = Age_data_select.withColumn('result',reduce(add, [F.col(x) for x in Age_data_select.columns[5:14]]))\n",
    "Age_data_total = Age_data_total.select(\"SA2_CODE_2021\",\"result\",\"Tot_P\")\n",
    "\n",
    "# Calculate the percentage of the targeted age group of all people\n",
    "Age_data_rate = Age_data_total.withColumn(\"age_percentage\", Age_data_total.result /Age_data_total.Tot_P)\n",
    "Age_data_rate = Age_data_rate.select(\"SA2_CODE_2021\",\"age_percentage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine income table\n",
    "income = census_17A_income.join(census_17B_income, census_17A_income.SA2_CODE_2021 == census_17B_income.SA2_CODE_2021).drop(census_17B_income.SA2_CODE_2021)\n",
    "income_data = income.join(census_17C_income, income.SA2_CODE_2021 == census_17C_income.SA2_CODE_2021).drop(census_17C_income.SA2_CODE_2021)\n",
    "\n",
    "# Select the column in the income that above Australia medium income and calculate the sum\n",
    "income_data_del = income_data.select(income_data.columns[0::10])\n",
    "income_data_select = income_data_del.withColumn('result',reduce(add, [F.col(x) for x in income_data_del.columns[44:50]]))\n",
    "income_data_total = income_data_select.select(\"SA2_CODE_2021\",\"result\",\"P_Tot_Tot\")\n",
    "\n",
    "# Calculate the percentage of the targeted income group of all people\n",
    "income_data_total = income_data_total.withColumn(\"income_percentage\", income_data_total.result / income_data_total.P_Tot_Tot)\n",
    "income_data_total = income_data_total.select(\"SA2_CODE_2021\",\"income_percentage\")\n",
    "\n",
    "# Combine income data and the age data to get final census data\n",
    "census_data = income_data_total.join(Age_data_rate, income_data_total.SA2_CODE_2021 == Age_data_rate.SA2_CODE_2021).drop(Age_data_rate.SA2_CODE_2021)\n",
    "census_data.write.mode('overwrite').parquet('../data/curated/final_census.parquet')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('ads')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6e45bdfcba1031802b2f7a315f02d536e51553942a60e4ee4be74aec82814bfa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
