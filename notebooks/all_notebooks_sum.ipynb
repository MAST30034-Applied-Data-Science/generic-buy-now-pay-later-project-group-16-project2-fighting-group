{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# !!Remember to add comma before the NOTES!! Organise the variable NAMES! MAGIC Numbers!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import zipfile\n",
    "import geopandas as gpd\n",
    "from dbfread import DBF\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.error import HTTPError\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/14 15:00:42 WARN Utils: Your hostname, hexiangyideMacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 10.13.189.29 instead (on interface en0)\n",
      "22/10/14 15:00:42 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/14 15:00:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/10/14 15:00:44 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 8) / 9]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/14 15:01:21 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "22/10/14 15:01:21 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "22/10/14 15:01:26 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# read the given transaction datasets\n",
    "ori_transaction1 = spark.read.parquet('../data/tables/transactions_20210228_20210827_snapshot').sort('order_datetime')\n",
    "ori_transaction2 = spark.read.parquet('../data/tables/transactions_20210828_20220227_snapshot').sort('order_datetime')\n",
    "ori_transaction3 = spark.read.parquet('../data/tables/transactions_20220228_20220828_snapshot').sort('order_datetime')\n",
    "\n",
    "# read the given consumer datasets\n",
    "tbl_consumer = spark.read.option(\"delimiter\", \"|\").option(\"header\",True).csv(\"../data/tables/tbl_consumer.csv\")\n",
    "consumer_detail = spark.read.parquet('../data/tables/consumer_user_details.parquet')\n",
    "\n",
    "# read the fraud dataset\n",
    "consumer_fraud = spark.read.option(\"header\",True).csv('../data/tables/consumer_fraud_probability.csv')\n",
    "\n",
    "# merge all the transaction data\n",
    "ori_transaction = ori_transaction1.union(ori_transaction2)\n",
    "ori_transaction = ori_transaction.union(ori_transaction3)\n",
    "ori_transaction = ori_transaction.drop('order_id')\n",
    "ori_transaction = ori_transaction.dropna(how='any')\n",
    "ori_transaction = ori_transaction.filter(\"order_datetime >= '2021-02-28'\")\n",
    "ori_transaction = ori_transaction.filter(\"order_datetime <= '2022-08-28'\")\n",
    "\n",
    "\n",
    "# delete outliers\n",
    "wind = Window.partitionBy('merchant_abn')\n",
    "q1= F.expr('percentile_approx(dollar_value, 0.25)')\n",
    "q3= F.expr('percentile_approx(dollar_value, 0.75)')\n",
    "testq = ori_transaction.withColumn('q1', q1.over(wind))\n",
    "testq = testq.withColumn('q3', q3.over(wind))\n",
    "testq = testq.withColumn('IQR', testq['q3']-testq['q1'])\n",
    "dele_out = testq.where((testq[\"dollar_value\"] <= testq[\"q1\"]+1.5*testq[\"IQR\"]) & (testq[\"dollar_value\"] >= testq[\"q1\"]-1.5*testq[\"IQR\"]))\n",
    "ori_transaction = dele_out.drop('q1','q3','IQR','order_id')\n",
    "\n",
    "# give a definition, if a single transaction is over $10000, this is a big order\n",
    "big_order_value = 10000\n",
    "ori_transaction = ori_transaction.withColumn(\n",
    "    \"whether_bigorder\",\n",
    "    F.when(F.col('dollar_value')>=big_order_value, 1).otherwise(0))\n",
    "\n",
    "# consumer datasets\n",
    "consumer = consumer_detail.join(tbl_consumer, consumer_detail.consumer_id == tbl_consumer.consumer_id).drop(tbl_consumer.consumer_id)\n",
    "consumer = consumer.select('user_id', 'postcode')\n",
    "\n",
    "# change to the date format\n",
    "consumer_fraud = consumer_fraud.select(col(\"user_id\"),col(\"fraud_probability\"),to_date(col(\"order_datetime\"),\"yyyy-MM-dd\").alias(\"date\"))\n",
    "\n",
    "# filter the range for the fraud data\n",
    "consumer_fraud = consumer_fraud.where(\n",
    "        # clean the data outside the date range\n",
    "        (F.col('date') >= '2021-02-28')& \n",
    "        (F.col('date') <= '2022-08-28'))\n",
    "\n",
    "consumer_fraud_grouped = consumer_fraud.groupBy('user_id').agg(F.avg('fraud_probability').alias('average_prob_con'))\n",
    "consumer_fraud_final = consumer_fraud_grouped.withColumn(\n",
    "    \"whether_fraud\",\n",
    "    F.when(F.col('average_prob_con')>=70, 1).otherwise(0))\n",
    "\n",
    "consumer_final = consumer.join(consumer_fraud_final, consumer.user_id == consumer_fraud_final.user_id).drop(consumer_fraud_final.user_id).fillna(0).fillna(0)\n",
    "consumer_final.write.parquet(f\"../data/curated/final_consumer.parquet\")\n",
    "\n",
    "transaction = ori_transaction.join(consumer_final, ori_transaction.user_id == consumer_final.user_id).drop(consumer_final.user_id)\n",
    "transaction = transaction.sort(transaction.user_id)\n",
    "transaction.write.parquet(f\"../data/curated/final_transaction.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final merchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/4w/p31d1t_x5pqbppcsm7p9kxt00000gn/T/ipykernel_8827/1964261070.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_merchants_pd['tags'].iloc[i] = tbl_merchants_pd['tags'].iloc[i].replace(r'[', r'(').replace(r']', r')')\n",
      "/var/folders/4w/p31d1t_x5pqbppcsm7p9kxt00000gn/T/ipykernel_8827/1964261070.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  tbl_merchants_pd['Store_type'][i] = ' '.join(tbl_merchants_pd['Store_type'][i].split())\n"
     ]
    }
   ],
   "source": [
    "# read the given merchant datasets\n",
    "merchant_fraud = spark.read.option(\"header\",True).csv('../data/tables/merchant_fraud_probability.csv')\n",
    "tbl_merchants = spark.read.parquet('../data/tables/tbl_merchants.parquet')\n",
    "\n",
    "# check the range of date\n",
    "\n",
    "merchant_fraud = merchant_fraud.select(col(\"merchant_abn\"),col(\"fraud_probability\"),to_date(col(\"order_datetime\"),\"yyyy-MM-dd\").alias(\"date\"))\n",
    "\n",
    "# filter the range for the fraud data\n",
    "merchant_fraud = merchant_fraud.where(\n",
    "        # clean the data outside the date range\n",
    "        (F.col('date') >= '2021-02-28')& \n",
    "        (F.col('date') <= '2022-08-28'))\n",
    "merchant_fraud_grouped = merchant_fraud.groupBy('merchant_abn').agg(F.count('fraud_probability').alias('fraud_count_abn'))\n",
    "\n",
    "# preprocess the given merchant datasets\n",
    "tbl_merchants_pd = tbl_merchants.toPandas()\n",
    "for i in range(int(tbl_merchants_pd['tags'].count())):\n",
    "    tbl_merchants_pd['tags'].iloc[i] = tbl_merchants_pd['tags'].iloc[i].replace(r'[', r'(').replace(r']', r')')\n",
    "tbl_merchants_pd['tags'] = tbl_merchants_pd['tags'].str.lower()\n",
    "\n",
    "# split the column into three columns and give names to the columns\n",
    "merchant_tags = tbl_merchants_pd['tags'].str.split(')', expand=True)\n",
    "for row in range(int(len(merchant_tags))):\n",
    "    for col in range(3):\n",
    "        merchant_tags.iloc[row,col] = merchant_tags.iloc[row,col].replace(r'((', r'').replace(r', (', r'').replace(r'take rate:', r'')\n",
    "merchant_tags.rename(columns = {0 : 'Store_type', 1 : 'Revenue_levels', 2 : 'Take_rate'}, inplace = True)\n",
    "merchant_tags = merchant_tags[['Store_type', 'Revenue_levels', 'Take_rate']]\n",
    "\n",
    "tbl_merchants_pd[['Store_type', 'Revenue_levels', 'Take_rate']] = merchant_tags[['Store_type', 'Revenue_levels', 'Take_rate']]\n",
    "\n",
    "\n",
    "for i in range(len(tbl_merchants_pd)):\n",
    "    tbl_merchants_pd['Store_type'][i] = ' '.join(tbl_merchants_pd['Store_type'][i].split())\n",
    "\n",
    "# export the merchant dataset\n",
    "merchants = tbl_merchants_pd[['merchant_abn', 'Store_type', 'Revenue_levels', 'Take_rate']]\n",
    "merchants.to_parquet(f\"../data/curated/final_merchant.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read external data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f8ed27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/10/14 15:01:49 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('id', 'string'), ('postcode', 'string'), ('locality', 'string'), ('state', 'string'), ('long', 'string'), ('lat', 'string'), ('dc', 'string'), ('type', 'string'), ('status', 'string'), ('sa3', 'string'), ('sa3name', 'string'), ('sa4', 'string'), ('sa4name', 'string'), ('region', 'string'), ('Lat_precise', 'string'), ('Long_precise', 'string'), ('SA1_MAINCODE_2011', 'string'), ('SA1_MAINCODE_2016', 'string'), ('SA2_MAINCODE_2016', 'string'), ('SA2_NAME_2016', 'string'), ('SA3_CODE_2016', 'string'), ('SA3_NAME_2016', 'string'), ('SA4_CODE_2016', 'string'), ('SA4_NAME_2016', 'string'), ('RA_2011', 'string'), ('RA_2016', 'string'), ('MMM_2015', 'string'), ('MMM_2019', 'string'), ('ced', 'string'), ('altitude', 'string'), ('chargezone', 'string'), ('phn_code', 'string'), ('phn_name', 'string'), ('lgaregion', 'string'), ('electorate', 'string'), ('electoraterating', 'string')]\n",
      "177\n",
      "complete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('postcode', 'string'), ('SA2_MAINCODE_2016', 'string'), ('avg_lat', 'double'), ('avg_long', 'double')]\n",
      "[('SA2_MAINCODE_2016', 'string'), ('SA2_NAME_2016', 'string'), ('SA2_CODE_2021', 'string'), ('SA2_NAME_2021', 'string'), ('RATIO_FROM_TO', 'string'), ('INDIV_TO_REGION_QLTY_INDICATOR', 'string'), ('OVERALL_QUALITY_INDICATOR', 'string'), ('BMOS_NULL_FLAG', 'string')]\n"
     ]
    }
   ],
   "source": [
    "#Go back one level from notebook to data \n",
    "output_relative_dir = '../data/'\n",
    "\n",
    "if not os.path.exists(output_relative_dir):\n",
    "    os.makedirs(output_relative_dir)\n",
    "    \n",
    "for target_dir in ('external','outer'):\n",
    "    if not os.path.exists(output_relative_dir + target_dir):\n",
    "        os.makedirs(output_relative_dir + target_dir)\n",
    "\n",
    "# Download census data\n",
    "url = \"https://www.abs.gov.au/census/find-census-data/datapacks/download/2021_GCP_SA2_for_AUS_short-header.zip\"#year-month.parquet\n",
    "output_dir = \"../data/outer/census.zip\"\n",
    "# download\n",
    "urlretrieve(url, output_dir) \n",
    "\n",
    "files = zipfile.ZipFile('../data/outer/census.zip','r')\n",
    "\n",
    "for file in files.namelist():\n",
    "    files.extract(file, f\"../data/outer/census\")\n",
    "\n",
    "types = [\"A\", \"B\"]\n",
    "for type in types:\n",
    "    G04= spark.read.option(\"header\",True).csv(f'../data/outer/census/2021 Census GCP Statistical Area 2 for AUS/2021Census_G04{type}_AUST_SA2.csv') #read the parquet \n",
    "    G04 = G04.na.drop(subset=G04.columns)\n",
    "    G04.write.mode(\"overwrite\").parquet(f\"../data/external/census_data_2021_04{type}.parquet\")\n",
    "\n",
    "types = [\"A\", \"B\", \"C\"]\n",
    "for type in types:\n",
    "    G17= spark.read.option(\"header\",True).csv(f'../data/outer/census/2021 Census GCP Statistical Area 2 for AUS/2021Census_G17{type}_AUST_SA2.csv') #read the parquet \n",
    "    G17 = G17.na.drop(subset=G17.columns)\n",
    "\n",
    "    G17.write.mode(\"overwrite\").parquet(f\"../data/external/census_data_2021_17{type}.parquet\")\n",
    "\n",
    "# Download postcode and SA2 data\n",
    "url = \"http://github.com/matthewproctor/australianpostcodes/zipball/master\"\n",
    "output_dir = \"../data/outer/total_pto2.zip\"\n",
    "# download\n",
    "urlretrieve(url, output_dir) \n",
    "\n",
    "files = zipfile.ZipFile('../data/outer/total_pto2.zip','r')\n",
    "for file in files.namelist():\n",
    "    files.extract(file, f\"../data/outer/total_pto2\")\n",
    "\n",
    "post_sa2= spark.read.option(\"header\",True).csv('../data/outer/total_pto2/matthewproctor-australianpostcodes-6f8a994/australian_postcodes.csv') \n",
    "print(post_sa2.dtypes)\n",
    "post_sa2 = post_sa2.select(\"id\", \"postcode\", \"Lat_precise\", \"Long_precise\", \"SA2_MAINCODE_2016\")\n",
    "print(post_sa2.count() - post_sa2.na.drop().count())\n",
    "post_sa2 = post_sa2.na.drop()\n",
    "# 177 rows with nulls dropped\n",
    "\n",
    "# get the average latitude and longitude of the postcode if it have duplicates\n",
    "post_sa2 = post_sa2\\\n",
    "    .withColumn(\"Lat_double\", post_sa2[\"Lat_precise\"].cast(\"double\"))\\\n",
    "    .withColumn(\"Long_double\", post_sa2[\"Long_precise\"].cast(\"double\"))\n",
    "post_sa2 = post_sa2\\\n",
    "    .groupBy(\"postcode\", \"SA2_MAINCODE_2016\")\\\n",
    "    .agg(F.avg(\"Lat_double\").alias(\"avg_lat\"), F.avg(\"Long_double\").alias(\"avg_long\"))\n",
    "\n",
    "post_sa2.write.mode(\"overwrite\").parquet(\"../data/external/postcode_SA2.parquet\")\n",
    "\n",
    "# Download SA2 2021 information\n",
    "url = \"https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/digital-boundary-files/SA2_2021_AUST_SHP_GDA2020.zip\" \n",
    "# year-month.parquet\n",
    "output_dir = \"../data/outer/2021sa2_shapefile.zip\"\n",
    "# download\n",
    "urlretrieve(url, output_dir) \n",
    "print(\"complete\")\n",
    "\n",
    "files = zipfile.ZipFile('../data/outer/2021sa2_shapefile.zip','r')\n",
    "for file in files.namelist():\n",
    "    files.extract(file, f\"../data/outer/2021sa2_shapefile\")\n",
    "\n",
    "# Download shapefile for each SA2 region\n",
    "# !!!WARNING!!! THIS CODE IS GOING TO TAKE 10 MINUTES\n",
    "def get_shapefile(url):\n",
    "    geojson_option = \"?_profile=oai&_mediatype=application/geo+json\"\n",
    "    try:\n",
    "        shape = str(gpd.read_file(url + geojson_option).iat[0,-1])\n",
    "    except HTTPError:\n",
    "        shape = \"\"\n",
    "    return shape\n",
    "\n",
    "get_shapefile_udf = udf(lambda a: get_shapefile(a),StringType())\n",
    "\n",
    "path = r'../data/outer/2021sa2_shapefile/SA2_2021_AUST_GDA2020.dbf' \n",
    "table = DBF(path)\n",
    "sa2_pd_temp = pd.DataFrame(iter(table))\n",
    "sa2_2021_temp = spark.createDataFrame(sa2_pd_temp) \n",
    "\n",
    "sa2_2021_vic = sa2_2021_temp.filter(F.col(\"STE_NAME21\")==\"Victoria\")\n",
    "sa2_2021_vic_w_geo = sa2_2021_vic.withColumn(\"geometry\", get_shapefile_udf(F.col(\"LOCI_URI21\")))\n",
    "sa2_2021_vic_w_geo = sa2_2021_vic_w_geo.select(\"SA2_CODE21\", \"SA2_NAME21\", \"geometry\")\n",
    "sa2_2021_vic_w_geo.write.option(\"maxRecordsPerFile\", 1).mode(\"overwrite\").parquet(\"../data/external/SA2_2021_VIC_shapefile.parquet\")\n",
    "\n",
    "# Download file with 2016 SA2 info and 2021 SA2 info\n",
    "url = \"https://www.abs.gov.au/statistics/standards/australian-statistical-geography-standard-asgs-edition-3/jul2021-jun2026/access-and-downloads/correspondences/CG_SA2_2016_SA2_2021.csv\"\n",
    "output_dir = \"../data/outer/correspondences.csv\"\n",
    "# download\n",
    "urlretrieve(url, output_dir) \n",
    "\n",
    "correspondences= spark.read.option(\"header\",True).csv('../data/outer/correspondences.csv') #read the parquet \n",
    "correspondences = correspondences.na.drop()\n",
    "correspondences.write.parquet(\"../data/external/correspondences.parquet\")\n",
    "\n",
    "# Find 2016 post code and 2021 sa2 correspondence\n",
    "print(post_sa2.dtypes)\n",
    "print(correspondences.dtypes)\n",
    "post_sa2_2021 = correspondences.join(post_sa2,correspondences.SA2_MAINCODE_2016 == post_sa2.SA2_MAINCODE_2016,\"left\") \n",
    "\n",
    "# JOIN tables\n",
    "post_sa2_2021 = post_sa2_2021.drop(\"SA2_NAME_2016\", \"SA2_MAINCODE_2016\")\n",
    "post_sa2_2021.write.mode(\"overwrite\").parquet(\"../data/external/postcode_sa2_conrrespondences.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## census_age&income"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the external census datasets\n",
    "census_04A_age = spark.read.parquet('../data/external/census_data_2021_04A.parquet')\n",
    "census_04B_age = spark.read.parquet('../data/external/census_data_2021_04B.parquet')\n",
    "census_17A_income = spark.read.parquet('../data/external/census_data_2021_17A.parquet')\n",
    "census_17B_income = spark.read.parquet('../data/external/census_data_2021_17B.parquet')\n",
    "census_17C_income = spark.read.parquet('../data/external/census_data_2021_17C.parquet')\n",
    "correspondences = spark.read.parquet('../data/external/correspondences.parquet')\n",
    "postcode = spark.read.parquet('../data/external/postcode_SA2.parquet/')\n",
    "\n",
    "# Merge the age dataframe\n",
    "Age_Data = census_04A_age.join(census_04B_age, census_04A_age.SA2_CODE_2021 == census_04B_age.SA2_CODE_2021).drop(census_04B_age.SA2_CODE_2021)\n",
    "\n",
    "# Select the column in the age dataframe (18-65) and calculate the sum\n",
    "Age_Data_select = Age_Data.select(Age_Data.columns[0::18])\n",
    "Age_Data_total = Age_Data_select.withColumn('result',reduce(add, [F.col(x) for x in Age_Data_select.columns[5:14]]))\n",
    "Age_Data_total = Age_Data_total.select(\"SA2_CODE_2021\",\"result\",\"Tot_P\")\n",
    "# Calculate the percentage of the targeted age group of all people\n",
    "Age_Data_rate = Age_Data_total.withColumn(\"age_percentage\", Age_Data_total.result /Age_Data_total.Tot_P)\n",
    "Age_Data_rate = Age_Data_rate.select(\"SA2_CODE_2021\",\"age_percentage\")\n",
    "# Combine income table\n",
    "income = census_17A_income.join(census_17B_income, census_17A_income.SA2_CODE_2021 == census_17B_income.SA2_CODE_2021).drop(census_17B_income.SA2_CODE_2021)\n",
    "income_data = income.join(census_17C_income, income.SA2_CODE_2021 == census_17C_income.SA2_CODE_2021).drop(census_17C_income.SA2_CODE_2021)\n",
    "# Select the column in the income that above Australia medium income and calculate the sum\n",
    "income_data_del = income_data.select(income_data.columns[0::10])\n",
    "income_data_select = income_data_del.withColumn('result',reduce(add, [F.col(x) for x in income_data_del.columns[44:50]]))\n",
    "income_Data_total = income_data_select.select(\"SA2_CODE_2021\",\"result\",\"P_Tot_Tot\")\n",
    "# Calculate the percentage of the targeted income group of all people\n",
    "income_Data_total = income_Data_total.withColumn(\"income_percentage\", income_Data_total.result / income_Data_total.P_Tot_Tot)\n",
    "income_Data_total = income_Data_total.select(\"SA2_CODE_2021\",\"income_percentage\")\n",
    "\n",
    "# Combine income data and the age data to get final census data\n",
    "census_data = income_Data_total.join(Age_Data_rate, income_Data_total.SA2_CODE_2021 == Age_Data_rate.SA2_CODE_2021).drop(Age_Data_rate.SA2_CODE_2021)\n",
    "census_data.write.parquet('../data/curated/final_census.parquet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "80d22fffded80c5bd95a70be935aa0a7eaf27817ba3bc2982545179d76b51d91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
