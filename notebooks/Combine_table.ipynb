{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "import numpy as np\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from dbfread import DBF\n",
    "from urllib.request import urlretrieve\n",
    "from urllib.error import HTTPError\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.functions import col\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import round"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the tables\n",
    "transaction = spark.read.parquet('../data/curated/final_transaction.parquet')\n",
    "merchants = spark.read.parquet('../data/curated/final_merchant.parquet')\n",
    "consumer = spark.read.parquet('../data/curated/final_consumer.parquet')\n",
    "census = spark.read.parquet('../data/curated/final_census.parquet')\n",
    "post_sa2_2021 = spark.read.parquet(\"../data/external/postcode_sa2_conrrespondences.parquet\")\n",
    "prediction = spark.read.parquet('../data/curated/prediction/predicted_dollar_value0.parquet')\n",
    "# Read all the prediction data\n",
    "for counts in range(1,45):\n",
    "    counts =  str(counts)\n",
    "    prediction_add = spark.read.parquet(f\"../data/curated/prediction/predicted_dollar_value{counts}.parquet\")\n",
    "    prediction = prediction.union(prediction_add)\n",
    "prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the tables, get the number of big order per merchant\n",
    "grouped_transaction = transaction.groupBy('merchant_abn').agg(\n",
    "    F.sum('dollar_value').alias('Amount'), F.count('dollar_value').alias('Count'), \n",
    "    F.sum('whether_bigorder').alias('count_of_bigorder')).sort('merchant_abn')\n",
    "grouped_transaction.drop(F.col('order_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get the monthly average features\n",
    "number_of_months = 18\n",
    "grouped_transaction = grouped_transaction.withColumn('Avg_amount_monthly', round(grouped_transaction['Amount']/number_of_months, 2))\n",
    "grouped_transaction = grouped_transaction.withColumn('Avg_count_monthly', round(grouped_transaction['Count']/number_of_months, 2))\n",
    "grouped_transaction = grouped_transaction.withColumn('Order_avg_value', round(grouped_transaction.Amount/grouped_transaction.Count,2))\n",
    "grouped_transaction = grouped_transaction.drop('Amount','Count')\n",
    "# Add monthly average features into merchants data\n",
    "merchant_data1 = merchants.join(grouped_transaction, merchants.merchant_abn == grouped_transaction.merchant_abn).drop(grouped_transaction.merchant_abn)\n",
    "merchant_data1 = merchant_data1.drop('Amount','Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the transaction data calculate each customer's fraud data\n",
    "ori_transaction_1 = transaction.groupby('merchant_abn','user_id').agg(\n",
    "    F.count('user_id').alias('count'), \n",
    "    F.avg('average_prob_con').alias('avg_prob_fraud_cus'),\n",
    "    F.avg('whether_fraud').alias('whether_fraud'))\n",
    "# Add the fraud data to merchants\n",
    "o_t = ori_transaction_1.groupby('merchant_abn').agg(\n",
    "    F.count('user_id').alias('cnt'), \n",
    "    F.avg('avg_prob_fraud_cus').alias('avg_prob_fraud_cus'),\n",
    "    F.sum('whether_fraud').alias('num_of_fraud'))\n",
    "# Calculate the probability of fraud customers among all customers\n",
    "cus_per_mon = o_t.withColumn('prob_of_fraud', o_t.num_of_fraud/o_t.cnt)\n",
    "cus_per_mon = cus_per_mon.withColumn('count_cus_per_mon', round(o_t['cnt']/number_of_months, 2))\n",
    "cus_per_mon = cus_per_mon.drop('cnt')\n",
    "cus_per_mon = cus_per_mon.drop('num_of_fraud')\n",
    "ori_transaction_2 = transaction.groupby('merchant_abn', 'user_id').count()\n",
    "# Calculate whether he/she is a regular customer\n",
    "ori_con_drop = ori_transaction_2.withColumn(\n",
    "    \"fixed_cus_num\",\n",
    "    F.when(F.col(\"count\") >= 5, 1).otherwise(0))\n",
    "# Calculate the number of the regular customer\n",
    "ori_con_fix = ori_con_drop.groupby('merchant_abn').agg(F.sum('fixed_cus_num').alias('fix_cus_num'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine the customer information into merchants\n",
    "user_info = cus_per_mon.join(ori_con_fix, cus_per_mon.merchant_abn == ori_con_fix.merchant_abn).drop(ori_con_fix.merchant_abn)\n",
    "user_info = user_info.drop('total_cus_num')\n",
    "merchant_abn_and_consumer_id = transaction['merchant_abn', 'user_id']\n",
    "user_id_and_postcode = consumer[['postcode','user_id']]\n",
    "merchant_and_consumer_postcode = merchant_abn_and_consumer_id.join(user_id_and_postcode,['user_id'])\n",
    "merchant_and_consumer_postcode = merchant_and_consumer_postcode['merchant_abn', 'postcode']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guess the merchants' postcode\n",
    "# https://stackoverflow.com/questions/36654162/mode-of-grouped-data-in-pyspark\n",
    "counts = merchant_and_consumer_postcode.groupBy(['merchant_abn', 'postcode']).count().alias('counts')\n",
    "merchant_postcode = (counts\n",
    "          .groupBy('merchant_abn')\n",
    "          .agg(F.max(F.struct(F.col('count'),\n",
    "                              F.col('postcode'))).alias('max'))\n",
    "          .select(F.col('merchant_abn'), F.col('max.postcode'))\n",
    "         )\n",
    "# Add census information into merchants\n",
    "merchant_info = merchant_data1.join(merchant_postcode, merchant_data1.merchant_abn == merchant_postcode.merchant_abn).drop(merchant_data1.merchant_abn)\n",
    "post_census = post_sa2_2021.join(census, post_sa2_2021.SA2_CODE_2021 == census.SA2_CODE_2021).drop(census.SA2_CODE_2021)\n",
    "post_census_1 = post_census.groupBy('postcode')\\\n",
    ".agg(F.avg(F.col('income_percentage')).alias(\"avg_income_percentage\"),\n",
    "F.avg(F.col('age_percentage')).alias('avg_age_percentage'))\n",
    "post_census_2 = post_census_1.select(post_census_1.postcode.cast('int'),post_census_1.avg_income_percentage, post_census_1.avg_age_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semifinal = merchant_info.join(user_info, merchant_info.merchant_abn == user_info.merchant_abn).drop(merchant_info.merchant_abn)\n",
    "final = semifinal.join(post_census_2, semifinal.postcode == post_census_2.postcode, 'left').drop(post_census_2.postcode)\n",
    "final = final.na.fill(value=0,subset=[\"avg_income_percentage\", \"avg_age_percentage\"])\n",
    "# Change take rate into predicted revenue for the BNPL companies\n",
    "final = final.withColumn('Take_rate', final.Take_rate*final.Avg_amount_monthly)\n",
    "final = final.join(prediction, final.merchant_abn == prediction.merchant_abn).drop(prediction.merchant_abn)\n",
    "final.write.parquet('../data/curated/merchant_info.parquet')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
